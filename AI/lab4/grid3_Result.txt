####################### Value Iteration ########################################
Value-Iteration converged at step 189.
[[ 0.49753776  0.63453584  0.79545987  1.        ]
 [ 0.39881724  0.49662784  0.48742762 -1.        ]
 [ 0.30912993  0.37955356  0.         -0.4       ]]
[['R' 'R' 'R' 'G']
 ['U' 'U' 'U' 'G']
 ['U' 'U' 'W' 'D']]
########################## Policy Iteration ####################################
Policy-Iteration converged at step 5.
Iter: 5
[[ 0.34848482  0.60184911  0.79217114  1.        ]
 [ 0.23930801  0.45672843  0.48146878 -1.        ]
 [ 0.1090908   0.31622332  0.         -0.222853  ]]
[['R' 'R' 'R' 'G']
 ['R' 'U' 'U' 'G']
 ['R' 'U' 'W' 'D']]
################################ TD Learning ###################################
[[ 0.49716289  0.68519715  0.85999907  1.        ]
 [-0.32760052 -0.73770603 -0.92835815 -1.        ]
 [-0.35942526 -0.50502486  0.          0.        ]]
[['R' 'R' 'R' 'G']
 ['R' 'R' 'R' 'G']
 ['R' 'R' 'W' 'R']]
######################### Q Learning ###########################################
[[ 0.61641842  0.72868434  0.87631485  1.        ]
 [ 0.49595933  0.57801039  0.55977536 -1.        ]
 [ 0.36472688  0.46600292  0.          0.        ]]
[['R' 'R' 'R' 'G']
 ['R' 'U' 'L' 'G']
 ['U' 'U' 'W' 'L']]