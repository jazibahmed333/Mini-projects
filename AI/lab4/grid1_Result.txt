
####################### Value Iteration ########################################
Value-Iteration converged at step 24.
[[ 0.5094156   0.64958636  0.79536224  1.        ]
 [ 0.39851125  0.          0.48644046 -1.        ]
 [ 0.29646654  0.25396055  0.3447884   0.12994247]]
[['R' 'R' 'R' 'G']
 ['U' 'W' 'U' 'G']
 ['U' 'R' 'U' 'L']]
########################## Policy Iteration ####################################
Policy-Iteration converged at step 5.
Iter: 5
[[ 0.39984038  0.62539304  0.79264275  1.        ]
 [ 0.26742185  0.          0.48293355 -1.        ]
 [ 0.14895235  0.196516    0.33360253  0.11839775]]
[['R' 'R' 'R' 'G']
 ['U' 'W' 'U' 'G']
 ['U' 'R' 'U' 'L']]
################################ TD Learning ###################################
[[ 0.46807757  0.69004957  0.85954784  1.        ]
 [ 0.4170298   0.         -0.93890383 -1.        ]
 [-0.73277916 -0.82120339 -0.81908491 -0.87166701]]
[['R' 'R' 'R' 'G']
 ['R' 'W' 'R' 'G']
 ['R' 'R' 'R' 'R']]
######################### Q Learning ###########################################
[[ 0.65139841  0.78155725  0.92519206  1.        ]
 [ 0.51892475  0.          0.68372169 -1.        ]
 [ 0.40392495  0.30929907  0.26951708  0.02404361]]
[['R' 'R' 'R' 'G']
 ['U' 'W' 'U' 'G']
 ['U' 'L' 'L' 'D']]